{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mb-EgxaWEaf9"
   },
   "source": [
    "# LOS Prediction - LSTM for Time-Based Data\n",
    "\n",
    "This notebook performs LOS prediction on data from the first X days from each patient. The goal is to utilize more of the dataset in order to see how the performance changes with respect to a standard DNN that only uses data from the first 24 or 48 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "bs0HiqKXEagF",
    "outputId": "beed1efe-12bf-4c8d-a6bf-cc9858011196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#\n",
    "# Imports Block\n",
    "#\n",
    "###\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "nCqs-Sw4Eagf",
    "outputId": "0b3d11dd-4174-4deb-c841-6e6f837489c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows in data: 51006\n",
      "Num columns before feature selection: 141\n",
      "Num columns after feature selection: 64\n",
      "Index(['age', 'Bicarbonate', 'Hematocrit', 'Hemoglobin', 'Lymphocytes',\n",
      "       'Monocytes', 'Platelet Count', 'Red Blood Cells', 'Urea Nitrogen',\n",
      "       'White Blood Cells', 'Calculated Total CO2', 'pCO2', 'pH', 'pO2',\n",
      "       'Phosphate', 'Oxygen Saturation', 'Oxygen', 'PEEP', 'Temperature',\n",
      "       'Tidal Volume', 'Albumin', 'Alkaline Phosphatase', 'Bilirubin, Total',\n",
      "       'Lactate Dehydrogenase (LD)', 'Potassium, Whole Blood', 'Free Calcium',\n",
      "       'NS250', 'PANT40I', 'ASA81', 'NS500', 'VIAL', 'ALBU3H', 'BISA5', 'MVI',\n",
      "       'DOCU100', 'NS/MBP100I', 'NS1000', 'PANT40', 'ONDAN4I', 'HEPA10SYR',\n",
      "       'SENN187', 'KCL20PM', 'D5W100', 'NYST5L', 'METO50', 'METO25', 'HEPA5I',\n",
      "       'NS100', 'MAG2PM', 'CHLO15L', 'CALG1I', 'LORA2I', 'IPRA2H', 'NACLFLUSH',\n",
      "       'BISA10R', 'D5W250', 'OXYC5', 'MAGS1I', 'KCLBASE2', 'ASA325',\n",
      "       'Systolic', 'O2 Flow.1', 'Temperature.1', 'O2'],\n",
      "      dtype='object')\n",
      "51006\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#\n",
    "# Data Import and Conversion to 3D Array\n",
    "#\n",
    "###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from math import sqrt\n",
    "\n",
    "# Consider imputing the data with values other than zero\n",
    "raw_dataset = pd.read_csv('/content/drive/My Drive/Stanford/CS221/timestep.filtered.csv', header=0, index_col=0)\n",
    "df = raw_dataset.copy()\n",
    "\n",
    "# One hot encode the demographic columns\n",
    "cols_to_one_hot = ['gender', 'insurance', 'language', 'religion', 'marital_status', 'ethnicity']\n",
    "for col in cols_to_one_hot:\n",
    "    one_hot = pd.get_dummies(df[col], col)\n",
    "    df = df.drop(col, axis = 1)\n",
    "    #if col == \"gender\" or col == \"insurance\":\n",
    "    #  df = df.join(one_hot)\n",
    "\n",
    "los = df.pop('los').values\n",
    "df.pop('status') # Not in use for this analysis\n",
    "timesteps = df.pop('timestep') # Temporarily removed so we don't scale this\n",
    "\n",
    "#\n",
    "# Manual feature selection \n",
    "#\n",
    "# print(\"Num rows in data: \" + str(len(df)))\n",
    "# print(\"Num columns before feature selection: \" + str(len(df.columns)))\n",
    "# df = df.filter(['Systolic', 'Diastolic', 'age', 'Anion Gap', 'Platelet Count', 'Heart Rate', 'O2-trend', 'Bicarbonate-trend', 'Phosphate', 'MCV-trend', 'White Blood Cells-trend', 'Neutrophils', 'Lactate', 'White Blood Cells', 'RDW', 'Lactate Dehydrogenase (LD)'])\n",
    "# print(\"Num columns after feature selection: \" + str(len(df.columns)))\n",
    "\n",
    "#\n",
    "# Normalize the dataset\n",
    "#\n",
    "train_stats = df.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "scaled_features = StandardScaler().fit_transform(df.values)\n",
    "df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\n",
    "\n",
    "#\n",
    "# Feature selection based on SGD\n",
    "#\n",
    "# print(\"Num rows in data: \" + str(len(df)))\n",
    "# print(\"Num columns before feature selection: \" + str(len(df.columns)))\n",
    "# clf = SGDRegressor(loss=\"squared_loss\", penalty=\"l1\", eta0=0.0001)\n",
    "# sfm = SelectFromModel(clf, threshold=0.18)\n",
    "# sfm.fit(df, los)\n",
    "# feature_idx = sfm.get_support()\n",
    "# df = df.loc[:,feature_idx]\n",
    "# print(\"Num columns after feature selection: \" + str(len(df.columns)))\n",
    "# print(df.columns)\n",
    "\n",
    "# df.insert(0, \"timestep\", timesteps, True)\n",
    "# print(len(df.values))\n",
    "\n",
    "\n",
    "#\n",
    "# Feature selection based on f_regression\n",
    "#\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print(\"Num rows in data: \" + str(len(df)))\n",
    "print(\"Num columns before feature selection: \" + str(len(df.columns)))\n",
    "sfm = SelectKBest(f_regression, k=64)\n",
    "sfm.fit(df, los)\n",
    "feature_idx = sfm.get_support()\n",
    "df = df.loc[:,feature_idx]\n",
    "print(\"Num columns after feature selection: \" + str(len(df.columns)))\n",
    "print(df.columns)\n",
    "\n",
    "df.insert(0, \"timestep\", timesteps, True)\n",
    "print(len(df.values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "yVMe_ncwEagt",
    "outputId": "ab601f18-98f3-4c9a-a866-c29a25dc6373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 3440\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "window_size = 5\n",
    "max_sequence_length = 10\n",
    "num_features = len(df.values[0]) - 1 # -1 accounts for the timestep column we are removing\n",
    "\n",
    "lstm_input = []\n",
    "lstm_los = []\n",
    "\n",
    "\n",
    "##### CREATE A SQUARE MATRIX REPRESENTING PATIENT SEQUENCES WITH ONE OUTPUT #####\n",
    "# 0,       0, x[0, 1], x[0, 2] -> y[0]\n",
    "# 0, x[1, 1], x[1, 2], x[1, 3] -> y[1]\n",
    "# This strategy uses stateless LSTM. We could use stateful LSTM and manually update at each timepoint in each patient \n",
    "# sequence, but this is extremely slow (note state is manually reset only after each patient sequence is finished). \n",
    "# https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM\n",
    "\n",
    "# Data frame is assumed to be sorted in order of timesteps for each patient\n",
    "vals = df.values\n",
    "i = 0\n",
    "while i < len(vals):\n",
    "    row = vals[i]\n",
    "    timestep = row[0]\n",
    "    remaining_los = los[i]\n",
    "    patient_sequence = []\n",
    "    start_i = i\n",
    "    i += 1\n",
    "    # Skip until the next patient sequence\n",
    "    while i < len(vals) and vals[i][0] != 0:\n",
    "        i += 1\n",
    "    end_i = i\n",
    "    \n",
    "    if end_i - start_i < window_size:\n",
    "        # Consider removing these patient samples\n",
    "        # Note: We need to front-pad the sequence with zeros in order to make sure we have a rectangular input\n",
    "        for _ in range(window_size - (end_i - start_i)):\n",
    "            patient_sequence.insert(0, [0] * num_features)\n",
    "        r = start_i\n",
    "        while r < end_i:\n",
    "            patient_sequence.append(vals[r, 1:])\n",
    "            r += 1\n",
    "\n",
    "        #######################################################\n",
    "        # COMMENT BELOW IF WE DON'T WANT TO KEEP SHORT SEQUENCES\n",
    "        #######################################################\n",
    "        #lstm_input.append(patient_sequence)\n",
    "        #lstm_los.append(remaining_los - window_size)\n",
    "    else:\n",
    "        # Create our sliding window data\n",
    "        for offset in range(min(max_sequence_length, (end_i - start_i) - window_size + 1)):\n",
    "            patient_sequence = []\n",
    "            r = start_i + offset\n",
    "            while r < start_i + offset + window_size:\n",
    "                patient_sequence.append(vals[r, 1:])\n",
    "                r += 1\n",
    "            lstm_input.append(patient_sequence)\n",
    "            lstm_los.append(remaining_los - offset - window_size)\n",
    "\n",
    "            #######################################################\n",
    "            # COMMENT BELOW IF WE WANT TO USE SLIDING WINDOW\n",
    "            #######################################################\n",
    "            break\n",
    "        \n",
    "lstm_input = np.array(lstm_input)\n",
    "lstm_los = np.array(lstm_los)\n",
    "lstm_los_cat = [[1 if a[0] else 0, 1 if a[1] else 0] for a in zip(lstm_los <= 10.75, lstm_los > 10.75)]\n",
    "lstm_los_cat = np.array(lstm_los_cat)\n",
    "\n",
    "#######################################################\n",
    "# UNCOMMENT BELOW IF WE WANT TO LIMIT NUM OF SAMPLES\n",
    "#######################################################\n",
    "# lstm_input = lstm_input[:2000, :]\n",
    "# lstm_los = lstm_los[:2000]\n",
    "\n",
    "print(\"Number of samples = \" + str(len(lstm_los)))\n",
    "#lstm_input[1]\n",
    "# lstm_input[0][0]\n",
    "# lstm_los[4]\n",
    "print(lstm_los_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "kITwVqSREagz",
    "outputId": "27a4786f-b0a4-4e8c-b913-8d91942656c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 5, 32)             12416     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 5, 32)             8320      \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 29,089\n",
      "Trainable params: 29,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "def build_model():\n",
    "  # Notes:\n",
    "  # - Use batch_input_shape=(batch_size, # timesteps, # features) param if using Stateful \n",
    "  #   (we're dynamically controlling the state at that point - ie. we can have varying sequence lengths)\n",
    "    \n",
    "  model = Sequential([\n",
    "    #layers.LSTM(32, input_shape=(window_size, num_features), return_sequences=True, stateful=False, dropout=0.3, recurrent_dropout=0.3, recurrent_regularizer=regularizers.l1_l2(0.01, 0.01)),\n",
    "    LSTM(32, input_shape=(window_size, num_features), return_sequences=True, dropout=0.5, stateful=False),\n",
    "    \n",
    "    LSTM(32, input_shape=(window_size, num_features), return_sequences=True, dropout=0.5, stateful=False),\n",
    "    \n",
    "    LSTM(32, input_shape=(window_size, num_features), dropout=0.5, stateful=False),\n",
    "    # layers.Dropout(0.5, name='dropout_2'),\n",
    "    #layers.Dense(64, activation='relu', input_shape=[num_features]),\n",
    "    #layers.Dropout(0.5, name='dropout_2'),\n",
    "    #layers.Dense(32, activation='relu', input_shape=[num_features]),\n",
    "    Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = 'adam'\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "def build_cnn():\n",
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(window_size,num_features)))\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(1))\n",
    "  model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "def build_cnn_cat():\n",
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(window_size,num_features)))\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(2, activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "# model = build_cnn_cat()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FqiZ5uCWEag5",
    "outputId": "833c28c8-b150-49a7-ebe5-936820386682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2408 samples, validate on 1032 samples\n",
      "Epoch 1/50\n",
      "2408/2408 [==============================] - 53s 22ms/step - loss: 136.5065 - mean_absolute_error: 7.3205 - mean_squared_error: 136.5065 - val_loss: 113.9636 - val_mean_absolute_error: 7.1666 - val_mean_squared_error: 113.9636\n",
      "Epoch 2/50\n",
      "2408/2408 [==============================] - 42s 18ms/step - loss: 125.7112 - mean_absolute_error: 7.0578 - mean_squared_error: 125.7112 - val_loss: 111.0577 - val_mean_absolute_error: 6.9427 - val_mean_squared_error: 111.0577\n",
      "Epoch 3/50\n",
      "2408/2408 [==============================] - 42s 17ms/step - loss: 123.6122 - mean_absolute_error: 6.9999 - mean_squared_error: 123.6122 - val_loss: 107.0332 - val_mean_absolute_error: 6.9059 - val_mean_squared_error: 107.0332\n",
      "Epoch 4/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 118.9969 - mean_absolute_error: 6.8365 - mean_squared_error: 118.9969 - val_loss: 106.8461 - val_mean_absolute_error: 6.7543 - val_mean_squared_error: 106.8461\n",
      "Epoch 5/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 117.2246 - mean_absolute_error: 6.7128 - mean_squared_error: 117.2246 - val_loss: 101.8998 - val_mean_absolute_error: 6.5692 - val_mean_squared_error: 101.8998\n",
      "Epoch 6/50\n",
      "2408/2408 [==============================] - 42s 17ms/step - loss: 115.9629 - mean_absolute_error: 6.7266 - mean_squared_error: 115.9629 - val_loss: 101.2289 - val_mean_absolute_error: 6.3844 - val_mean_squared_error: 101.2289\n",
      "Epoch 7/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 114.4451 - mean_absolute_error: 6.6651 - mean_squared_error: 114.4451 - val_loss: 100.9376 - val_mean_absolute_error: 6.7301 - val_mean_squared_error: 100.9376\n",
      "Epoch 8/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 111.5087 - mean_absolute_error: 6.6120 - mean_squared_error: 111.5087 - val_loss: 101.6117 - val_mean_absolute_error: 5.9674 - val_mean_squared_error: 101.6117\n",
      "Epoch 9/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 109.4267 - mean_absolute_error: 6.5801 - mean_squared_error: 109.4267 - val_loss: 98.1291 - val_mean_absolute_error: 6.2627 - val_mean_squared_error: 98.1291\n",
      "Epoch 10/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 106.6038 - mean_absolute_error: 6.4776 - mean_squared_error: 106.6038 - val_loss: 101.5452 - val_mean_absolute_error: 6.1248 - val_mean_squared_error: 101.5452\n",
      "Epoch 11/50\n",
      "2408/2408 [==============================] - 42s 17ms/step - loss: 107.1728 - mean_absolute_error: 6.4798 - mean_squared_error: 107.1728 - val_loss: 105.9792 - val_mean_absolute_error: 6.1655 - val_mean_squared_error: 105.9792\n",
      "Epoch 12/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 107.3244 - mean_absolute_error: 6.4712 - mean_squared_error: 107.3244 - val_loss: 103.4597 - val_mean_absolute_error: 6.0960 - val_mean_squared_error: 103.4597\n",
      "Epoch 13/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 102.4157 - mean_absolute_error: 6.3045 - mean_squared_error: 102.4157 - val_loss: 103.1209 - val_mean_absolute_error: 6.2783 - val_mean_squared_error: 103.1209\n",
      "Epoch 14/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 103.7551 - mean_absolute_error: 6.3837 - mean_squared_error: 103.7551 - val_loss: 98.8751 - val_mean_absolute_error: 6.1022 - val_mean_squared_error: 98.8751\n",
      "Epoch 15/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 104.5512 - mean_absolute_error: 6.3907 - mean_squared_error: 104.5512 - val_loss: 101.2853 - val_mean_absolute_error: 5.9924 - val_mean_squared_error: 101.2853\n",
      "Epoch 16/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 101.9172 - mean_absolute_error: 6.3095 - mean_squared_error: 101.9172 - val_loss: 102.9536 - val_mean_absolute_error: 6.4111 - val_mean_squared_error: 102.9536\n",
      "Epoch 17/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 99.4073 - mean_absolute_error: 6.2679 - mean_squared_error: 99.4073 - val_loss: 101.8185 - val_mean_absolute_error: 6.1517 - val_mean_squared_error: 101.8185\n",
      "Epoch 18/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 101.6713 - mean_absolute_error: 6.2541 - mean_squared_error: 101.6713 - val_loss: 102.8356 - val_mean_absolute_error: 6.1048 - val_mean_squared_error: 102.8356\n",
      "Epoch 19/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 98.5822 - mean_absolute_error: 6.2073 - mean_squared_error: 98.5822 - val_loss: 100.1552 - val_mean_absolute_error: 6.1649 - val_mean_squared_error: 100.1552\n",
      "Epoch 20/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 100.4231 - mean_absolute_error: 6.2345 - mean_squared_error: 100.4231 - val_loss: 97.8660 - val_mean_absolute_error: 6.2087 - val_mean_squared_error: 97.8660\n",
      "Epoch 21/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 101.5213 - mean_absolute_error: 6.2975 - mean_squared_error: 101.5213 - val_loss: 96.7244 - val_mean_absolute_error: 5.9525 - val_mean_squared_error: 96.7244\n",
      "Epoch 22/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 95.6593 - mean_absolute_error: 6.1335 - mean_squared_error: 95.6593 - val_loss: 100.8630 - val_mean_absolute_error: 5.8849 - val_mean_squared_error: 100.8630\n",
      "Epoch 23/50\n",
      "2408/2408 [==============================] - 40s 16ms/step - loss: 97.7205 - mean_absolute_error: 6.1507 - mean_squared_error: 97.7205 - val_loss: 100.0092 - val_mean_absolute_error: 5.9271 - val_mean_squared_error: 100.0092\n",
      "Epoch 24/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 97.3215 - mean_absolute_error: 6.0923 - mean_squared_error: 97.3215 - val_loss: 99.9387 - val_mean_absolute_error: 6.2403 - val_mean_squared_error: 99.9387\n",
      "Epoch 25/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 97.3183 - mean_absolute_error: 6.0635 - mean_squared_error: 97.3183 - val_loss: 106.4579 - val_mean_absolute_error: 6.0504 - val_mean_squared_error: 106.4579\n",
      "Epoch 26/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 94.2740 - mean_absolute_error: 6.0748 - mean_squared_error: 94.2740 - val_loss: 105.9662 - val_mean_absolute_error: 6.0934 - val_mean_squared_error: 105.9662\n",
      "Epoch 27/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 95.5823 - mean_absolute_error: 6.1329 - mean_squared_error: 95.5823 - val_loss: 102.6750 - val_mean_absolute_error: 6.0874 - val_mean_squared_error: 102.6750\n",
      "Epoch 28/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 97.6225 - mean_absolute_error: 6.1564 - mean_squared_error: 97.6225 - val_loss: 100.4592 - val_mean_absolute_error: 5.9746 - val_mean_squared_error: 100.4592\n",
      "Epoch 29/50\n",
      "2408/2408 [==============================] - 39s 16ms/step - loss: 90.8827 - mean_absolute_error: 6.0889 - mean_squared_error: 90.8827 - val_loss: 101.7019 - val_mean_absolute_error: 5.9178 - val_mean_squared_error: 101.7019\n",
      "Epoch 30/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 90.5082 - mean_absolute_error: 6.0093 - mean_squared_error: 90.5082 - val_loss: 101.4205 - val_mean_absolute_error: 6.0256 - val_mean_squared_error: 101.4205\n",
      "Epoch 31/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 93.4052 - mean_absolute_error: 6.0678 - mean_squared_error: 93.4052 - val_loss: 100.9302 - val_mean_absolute_error: 5.9858 - val_mean_squared_error: 100.9302\n",
      "Epoch 32/50\n",
      "2408/2408 [==============================] - 39s 16ms/step - loss: 94.0846 - mean_absolute_error: 6.0735 - mean_squared_error: 94.0846 - val_loss: 102.5546 - val_mean_absolute_error: 5.9805 - val_mean_squared_error: 102.5546\n",
      "Epoch 33/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 88.6683 - mean_absolute_error: 5.8799 - mean_squared_error: 88.6683 - val_loss: 102.1855 - val_mean_absolute_error: 5.9409 - val_mean_squared_error: 102.1855\n",
      "Epoch 34/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 93.8582 - mean_absolute_error: 5.9842 - mean_squared_error: 93.8582 - val_loss: 107.1545 - val_mean_absolute_error: 6.2006 - val_mean_squared_error: 107.1545\n",
      "Epoch 35/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 88.8498 - mean_absolute_error: 5.9972 - mean_squared_error: 88.8498 - val_loss: 103.3432 - val_mean_absolute_error: 6.0709 - val_mean_squared_error: 103.3432\n",
      "Epoch 36/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 92.9107 - mean_absolute_error: 6.0427 - mean_squared_error: 92.9107 - val_loss: 105.0218 - val_mean_absolute_error: 6.0875 - val_mean_squared_error: 105.0218\n",
      "Epoch 37/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 90.6591 - mean_absolute_error: 5.9802 - mean_squared_error: 90.6591 - val_loss: 104.6195 - val_mean_absolute_error: 6.0681 - val_mean_squared_error: 104.6195\n",
      "Epoch 38/50\n",
      "2408/2408 [==============================] - 39s 16ms/step - loss: 92.5144 - mean_absolute_error: 5.9669 - mean_squared_error: 92.5144 - val_loss: 107.4861 - val_mean_absolute_error: 6.0245 - val_mean_squared_error: 107.4861\n",
      "Epoch 39/50\n",
      "2408/2408 [==============================] - 39s 16ms/step - loss: 89.9651 - mean_absolute_error: 6.0105 - mean_squared_error: 89.9651 - val_loss: 105.2284 - val_mean_absolute_error: 6.0714 - val_mean_squared_error: 105.2284\n",
      "Epoch 40/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 91.9949 - mean_absolute_error: 6.0183 - mean_squared_error: 91.9949 - val_loss: 108.1459 - val_mean_absolute_error: 6.1657 - val_mean_squared_error: 108.1459\n",
      "Epoch 41/50\n",
      "2408/2408 [==============================] - 39s 16ms/step - loss: 89.0158 - mean_absolute_error: 5.9088 - mean_squared_error: 89.0158 - val_loss: 105.7984 - val_mean_absolute_error: 6.0920 - val_mean_squared_error: 105.7984\n",
      "Epoch 42/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 86.4027 - mean_absolute_error: 5.9996 - mean_squared_error: 86.4027 - val_loss: 104.9654 - val_mean_absolute_error: 6.0752 - val_mean_squared_error: 104.9654\n",
      "Epoch 43/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 88.8331 - mean_absolute_error: 6.0053 - mean_squared_error: 88.8331 - val_loss: 102.6287 - val_mean_absolute_error: 5.9023 - val_mean_squared_error: 102.6287\n",
      "Epoch 44/50\n",
      "2408/2408 [==============================] - 40s 16ms/step - loss: 87.2129 - mean_absolute_error: 5.9438 - mean_squared_error: 87.2129 - val_loss: 104.5036 - val_mean_absolute_error: 6.0171 - val_mean_squared_error: 104.5036\n",
      "Epoch 45/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 86.5922 - mean_absolute_error: 5.9572 - mean_squared_error: 86.5922 - val_loss: 106.2207 - val_mean_absolute_error: 6.1464 - val_mean_squared_error: 106.2207\n",
      "Epoch 46/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 84.1198 - mean_absolute_error: 5.9497 - mean_squared_error: 84.1198 - val_loss: 105.8470 - val_mean_absolute_error: 6.1080 - val_mean_squared_error: 105.8470\n",
      "Epoch 47/50\n",
      "2408/2408 [==============================] - 40s 16ms/step - loss: 86.8942 - mean_absolute_error: 5.9383 - mean_squared_error: 86.8942 - val_loss: 105.2211 - val_mean_absolute_error: 6.0852 - val_mean_squared_error: 105.2211\n",
      "Epoch 48/50\n",
      "2408/2408 [==============================] - 41s 17ms/step - loss: 82.6170 - mean_absolute_error: 5.8698 - mean_squared_error: 82.6170 - val_loss: 107.1347 - val_mean_absolute_error: 6.1346 - val_mean_squared_error: 107.1347\n",
      "Epoch 49/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 78.8625 - mean_absolute_error: 5.7128 - mean_squared_error: 78.8625 - val_loss: 112.5515 - val_mean_absolute_error: 6.3101 - val_mean_squared_error: 112.5515\n",
      "Epoch 50/50\n",
      "2408/2408 [==============================] - 40s 17ms/step - loss: 84.1059 - mean_absolute_error: 5.8578 - mean_squared_error: 84.1059 - val_loss: 107.2415 - val_mean_absolute_error: 6.1762 - val_mean_squared_error: 107.2415\n"
     ]
    }
   ],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# # Batch size defaults to 32\n",
    "# history = model.fit(\n",
    "#   lstm_input, lstm_los,\n",
    "#   batch_size=1,\n",
    "#   epochs=EPOCHS, validation_split = 0.3, verbose=1,\n",
    "#   callbacks=[PrintDot()])\n",
    "\n",
    "#### CAT ####\n",
    "# Batch size defaults to 32\n",
    "history = model.fit(\n",
    "  lstm_input, lstm_los,\n",
    "  batch_size=1,\n",
    "  epochs=EPOCHS, validation_split = 0.3, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R1ccn5PREag-",
    "outputId": "2f9306e0-f3d6-4427-da2c-5f6ad313fae9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.174688288051363"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()\n",
    "hist[\"val_mean_absolute_error\"][3:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "6WMHRCnnEahD",
    "outputId": "e24b657c-f8bb-438d-ba46-18508bb88291"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPzMRzRNn49PxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [los]')\n",
    "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,max(hist['val_mean_absolute_error'])])\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "syco1HbXEahM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lstm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
